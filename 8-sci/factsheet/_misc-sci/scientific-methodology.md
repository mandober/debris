# Scientific methodology

https://www.acm.org/publications/policies/artifact-review-badging

## 1. Contents

<!-- TOC -->

- [1. Contents](#1-contents)
- [2. Scientific method](#2-scientific-method)
- [3. Repeatability of results](#3-repeatability-of-results)
  - [3.1. Repeatability](#31-repeatability)
  - [3.2. Reproducibility](#32-reproducibility)
  - [3.3. Replicability](#33-replicability)
- [4. Badging](#4-badging)
  - [4.1. Artifacts Evaluated](#41-artifacts-evaluated)
    - [4.1.1. Artifacts Evaluated: Functional](#411-artifacts-evaluated-functional)
    - [4.1.2. Artifacts Evaluated: Reusable](#412-artifacts-evaluated-reusable)
  - [4.2. Artifacts Available](#42-artifacts-available)
  - [4.3. Results Validated](#43-results-validated)
    - [4.3.1. Results Reproduced](#431-results-reproduced)
    - [4.3.2. Results Replicated](#432-results-replicated)

<!-- /TOC -->

## 2. Scientific method

An experimental result is not fully established unless it can be independently reproduced.

A variety of recent studies, primarily in the biomedical field, have revealed that an uncomfortably large number of research results found in the literature fail this test, because of sloppy experimental methods, flawed statistical analyses, even fraud.

Publishers can promote the integrity of the research ecosystem by developing review processes that increase the likelihood that results can be independently replicated and reproduced.

An *extreme approach* would be to require completely independent reproduction of results as part of the refereeing process. An *intermediate approach* is to require that artifacts associated with the work undergo a formal audit.

By **artifact** we mean a digital object that was either created by the authors to be used as part of the study or generated by the experiment itself. For example, artifacts can be software systems, scripts used to run experiments, input datasets, raw data collected in the experiment, or scripts used to analyze results.

Additional benefits ensue if the research artifacts are themselves made publically available so that any interested party may audit them. This also enables replication experiments to be performed, which, because they inevitably are done under slightly different conditions, serve to verify the robustness of the original results. And perhaps more importantly, well-formed and documented artifacts allow others to build directly upon the previous work through reuse and repurposing.

## 3. Repeatability of results

*Repeatability* is something we expect of any well-controlled experiment. Results that are not repeatable are rarely suitable for publication.

The proposed intermediate concept of *replicability* stems from the unique properties of *computational experiments*, i.e., that the measurement procedure/system, being virtual, is more easily portable, enabling inspection and exercise by others. While *reproducibility* is the ultimate goal, this initiative seeks to take an intermediate step, that is, to promote practices that lead to better replicability.

We fully acknowledge that simple replication of results using author-supplied artifacts is a weak form of reproducibility. Nevertheless, it is an important first step, and the auditing processes that go well beyond traditional refereeing will begin to raise the bar for experimental research in computing.

### 3.1. Repeatability

(same team, same experimental setup)    
The measurement can be obtained with stated precision by
- the same team
- using the same measurement procedure
- the same measuring system
- under the same operating conditions
- in the same location
- on multiple trials
> For computational experiments, this means that 
> a researcher can reliably repeat her own computation.


### 3.2. Reproducibility

(different team, different experimental setup)    
The measurement can be obtained with stated precision by
- a different team
- using the same measurement procedure
- the same measuring system
- under the same operating conditions
- in the same or a different location
- on multiple trials
> For computational experiments, this means that 
> an independent group can obtain the same result 
> using the author's own artifacts.


### 3.3. Replicability

(different team, same experimental setup)    
The measurement can be obtained with stated precision by
- a different team
- a different measuring system
- in a different location
- on multiple trials
> For computational experiments, this means that 
> an independent group can obtain the same result 
> using artifacts which they develop completely independently.


## 4. Badging

We recommend that 3 separate badges related to artifact review be associated with research articles in ACM publications:
- Artifacts Evaluated
  - Artifacts Evaluated: Functional
  - Artifacts Evaluated: Reusable
- Artifacts Available
- Results Validated
  - Results Reproduced
  - Results Replicated


![Artifacts Evaluated - Functional](artifacts_evaluated_functional_dl.jpg)
![Artifacts Evaluated - Reusable](artifacts_evaluated_reusable_dl.jpg)
![Artifacts Available](artifacts_available_dl.jpg)
![Results Reproduced](results_reproduced_dl.jpg)
![Results Replicated](results_replicated_dl.jpg)

These badges are considered independent and any one, two or all three can be applied to any given paper depending on review procedures developed by the journal or conference.

### 4.1. Artifacts Evaluated

This badge is applied to papers whose associated artifacts have successfully completed an independent audit. Artifacts need not be made publicly available to be considered for this badge. However, they do need to be made available to reviewers.

Two levels are distinguished, only one of which should be applied in any instance:

#### 4.1.1. Artifacts Evaluated: Functional

![Artifacts Evaluated - Functional](artifacts_evaluated_functional_dl.jpg)

The artifacts associated with the research are found to be documented, consistent, complete, exercisable, and include appropriate evidence of verification and validation.

* Documented: At minimum, an inventory of artifacts is included, and sufficient description provided to enable the artifacts to be exercised.

* Consistent: The artifacts are relevant to the associated paper, and contribute in some inherent way to the generation of its main results.

* Complete: To the extent possible, all components relevant to the paper in question are included. (Proprietary artifacts need not be included. If they are required to exercise the package then this should be documented, along with instructions on how to obtain them. Proxies for proprietary data should be included so as to demonstrate the analysis.)

* Exercisable: Included scripts and/or software used to generate the results in the associated paper can be successfully executed, and included data can be accessed and appropriately manipulated.

#### 4.1.2. Artifacts Evaluated: Reusable

![Artifacts Evaluated - Reusable](artifacts_evaluated_reusable_dl.jpg)

The artifacts associated with the paper are of a quality that significantly exceeds minimal functionality. That is, they have all the qualities of the *Artifacts Evaluated - Functional* level, but, in addition, they are very carefully documented and well-structured to the extent that reuse and repurposing is facilitated. In particular, norms and standards of the research community for artifacts of this type are strictly adhered to.

### 4.2. Artifacts Available

![Artifacts Available](artifacts_available_dl.jpg)

Author-created artifacts relevant to this paper have been placed on a publically accessible archival repository. A DOI or link to this repository along with a unique identifier for the object is provided.

Artifacts do not need to have been formally evaluated in order for an article to receive this badge. In addition, they need not be complete; they simply need to be relevant to the study and add value beyond the text in the article. Such artifacts could be something as simple as the data from which the figures are drawn, or as complex as a complete software system under study.

### 4.3. Results Validated

This badge is applied to papers in which the main results of the paper have been successfully obtained by a person or team other than the author.

Two levels are distinguished:
- Results Reproduced
- Results Replicated

#### 4.3.1. Results Reproduced

![Results Reproduced](results_reproduced_dl.jpg)

The main results of the paper have been independently obtained in a subsequent study by a person or team other than the authors, without the use of author-supplied artifacts.

#### 4.3.2. Results Replicated

![Results Replicated](results_replicated_dl.jpg)

The main results of the paper have been obtained in a subsequent study by a person or team other than the authors, using, in part, artifacts provided by the author.
